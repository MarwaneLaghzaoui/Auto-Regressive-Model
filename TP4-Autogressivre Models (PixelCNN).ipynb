{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Models (PixelCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# Parameters\n",
    "IMAGE_SIZE = 16\n",
    "PIXEL_LEVELS = 4\n",
    "N_FILTERS = 128\n",
    "RESIDUAL_BLOCKS = 5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 150\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (x * 255).float() // (256 // PIXEL_LEVELS))\n",
    "])\n",
    "path= \"./data\"\n",
    "dataset = datasets.FashionMNIST(root=path, train=True, download=True, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Convolution Layer\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "        _, _, h, w = self.weight.size()\n",
    "        self.mask.fill_(1)\n",
    "        self.mask[:, :, h // 2, w // 2 + (mask_type == 'B'):] = 0\n",
    "        self.mask[:, :, h // 2 + 1:] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super().forward(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(filters, filters // 2, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            MaskedConv2d('B', filters // 2, filters // 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(filters // 2, filters, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PixelCNN Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PixelCNN Model\n",
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            MaskedConv2d('A', 1, N_FILTERS, kernel_size=7, padding=3),\n",
    "            *[ResidualBlock(N_FILTERS) for _ in range(RESIDUAL_BLOCKS)],\n",
    "            nn.ReLU(),\n",
    "            MaskedConv2d('B', N_FILTERS, N_FILTERS, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            MaskedConv2d('B', N_FILTERS, N_FILTERS, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(N_FILTERS, PIXEL_LEVELS, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = PixelCNN().to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 469/469 [04:08<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, _ in tqdm(data_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        \n",
    "        images = images.to(DEVICE).squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        inputs = images.float().unsqueeze(1)     # [batch, 1, H, W]\n",
    "        targets = images.long()                  # [batch, H, W]\n",
    "        outputs = model(inputs)                  # [batch, PIXEL_LEVELS, H, W]       \n",
    "        loss = criterion(outputs, targets)       # CrossEntropy expects float inputs and long targets \n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        total_loss += loss.item()   \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in Model\n"
     ]
    }
   ],
   "source": [
    "# load in model weights and set to eval mode\n",
    "save_dir = './models'\n",
    "model.load_state_dict(torch.load(f\"{save_dir}/PixelCNN.pt\", map_location=DEVICE))\n",
    "model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated images shape: (10, 1, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "def generate_images(model, num_images, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = torch.zeros(num_images, 1, IMAGE_SIZE, IMAGE_SIZE, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        for row in range(IMAGE_SIZE):\n",
    "            for col in range(IMAGE_SIZE):\n",
    "                logits = model(generated.float())[:, :, row, col] / temperature\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                generated[:, 0, row, col] = torch.multinomial(probs, 1).squeeze(-1)\n",
    "    return generated.cpu().numpy() / PIXEL_LEVELS\n",
    "\n",
    "# Generate sample images\n",
    "sample_images = generate_images(model, num_images=10)\n",
    "print(\"Generated images shape:\", sample_images.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9wAAAGGCAYAAACJ2omlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAPD0lEQVR4nO3dUY6cPAKFUWrEFs12MNvBi6xRZh5G+TWKyum+jQ3nPDtuFwWmPvkhr/f7/V4AAACAb/Wv750OAAAA+EVwAwAAQIDgBgAAgADBDQAAAAGCGwAAAAIENwAAAAQIbgAAAAgQ3AAAABAguAEAACBg/XTg6/VK/P1HKKXE5j7PMzJva20ZQXIdX712ngnu5v1+f+nfeybGVGtdZjPKmj0TJKV+w438m3bGZ2KU/WiUddD/nnDCDQAAAAGCGwAAAAIENwAAAAQIbgAAAAgQ3AAAABAguAEAACBAcAMAAECA4AYAAIAAwQ0AAAABghsAAAAC1sSkT1BK+XjseZ7LnT/fSOtorcXWAjCCWmt0PPCs31pP/O3Usy/u+77Mxr4/FifcAAAAECC4AQAAIEBwAwAAQIDgBgAAgADBDQAAAAGCGwAAAAIENwAAAAQIbgAAAAgQ3AAAABAguAEAACBAcAMAAEDAutxYKSU6nr/TWot9J8m5AUZQa11mXMco64aUWX9XPPG3077vVy+BB3HCDQAAAAGCGwAAAAIENwAAAAQIbgAAAAgQ3AAAABAguAEAACBAcAMAAECA4AYAAIAAwQ0AAAABghsAAAAC1mUypZTI2OQ6ZtRai81992sH8AS11iHmTq4DmMfd94J932Nz3/3aXc0JNwAAAAQIbgAAAAgQ3AAAABAguAEAACBAcAMAAECA4AYAAIAAwQ0AAAABghsAAAACBDcAAAAECG4AAAAIWBOTMr/WWmzuUkpsbgDGVGv9eOy+75F5f1rP+y753uVZeu+lu9x7qT1mFMdx3GJffCIn3AAAABAguAEAACBAcAMAAECA4AYAAIAAwQ0AAAABghsAAAACBDcAAAAECG4AAAAIENwAAAAQILgBAAAgQHADAABAwJqYdFattauXsJRSlhEk15G8ziN8h3yvWe/FJ6q1Xr0EBrbv+xD33RPv01F+W8y457p2zLiHPnGfG5kTbgAAAAgQ3AAAABAguAEAACBAcAMAAECA4AYAAIAAwQ0AAAABghsAAAACBDcAAAAECG4AAAAIENwAAAAQsCYmndV5nlcvYWmtdY0vpUTW0Ttv77qvnpd5pO7x3rl778XUvZu8Hj9p3/erl8BN1FqvXgKB31nbtg2xjp69/C77M787jmOI91vvOhiHE24AAAAIENwAAAAQILgBAAAgQHADAABAgOAGAACAAMENAAAAAYIbAAAAAgQ3AAAABAhuAAAACBDcAAAAELAmJp1Va+3jsaWUyLxPWHOPUdbB9zrPc5lNz/PzN+Pv8AzVWmNz7/semxsYbz8a5T3Rs+be/XaUz8ifef/wVU64AQAAIEBwAwAAQIDgBgAAgADBDQAAAAGCGwAAAAIENwAAAAQIbgAAAAgQ3AAAABAguAEAACBAcAMAAEDAmpj0CVpry2xmXDPzKKVcvQQmUmuNje+dO2Xf92U2x3FcvQQu9oS9fNu25c783ptnH53xPUE/J9wAAAAQILgBAAAgQHADAABAgOAGAACAAMENAAAAAYIbAAAAAgQ3AAAABAhuAAAACBDcAAAAECC4AQAAIEBwAwAAQMC6TKaUcvUS4DHO87x6CdCt1rqMYJR1QFJr7fLfcD1rgLR93y+f1/tnLE64AQAAIEBwAwAAQIDgBgAAgADBDQAAAAGCGwAAAAIENwAAAAQIbgAAAAgQ3AAAABAguAEAACBAcAMAAEDAmpgU/qS1dvUSbqWUEhnL1/Rc6+Qz4TsHemzbNsRe94TfCqnP+IRr91XHcXw8dt/3ZbY1z6rWGhl7NSfcAAAAECC4AQAAIEBwAwAAQIDgBgAAgADBDQAAAAGCGwAAAAIENwAAAAQIbgAAAAgQ3AAAABAguAEAACBgXSbTWvt4bCkluhby3+FdJO9F9/mYRrnP7ZlwnVH2gaQnfMYU147jOK5ewlBqrcsdOeEGAACAAMENAAAAAYIbAAAAAgQ3AAAABAhuAAAACBDcAAAAECC4AQAAIEBwAwAAQIDgBgAAgADBDQAAAAGCGwAAAALWxKTA71prQ8xdSomtgzG/w+S9B1ynZy+wD8DfO47j6iUwOSfcAAAAECC4AQAAIEBwAwAAQIDgBgAAgADBDQAAAAGCGwAAAAIENwAAAAQIbgAAAAgQ3AAAABAguAEAACBgTUzK87TWrl4CH/A9zc93CPQqpXw81h4Dv9v3vWv8cRyRuXvmZSxOuAEAACBAcAMAAECA4AYAAIAAwQ0AAAABghsAAAACBDcAAAAECG4AAAAIENwAAAAQILgBAAAgQHADAABAwJqYFACA+ZRSlhG01q5eAvzHcRzLbGqt0fH0ccINAAAAAYIbAAAAAgQ3AAAABAhuAAAACBDcAAAAECC4AQAAIEBwAwAAQIDgBgAAgADBDQAAAAGCGwAAAAIENwAAAASsy4211rrGl1JiawEAAO6t1vrx2H3fo2thDE64AQAAIEBwAwAAQIDgBgAAgADBDQAAAAGCGwAAAAIENwAAAAQIbgAAAAgQ3AAAABAguAEAACBAcAMAAEDA6/1+vxMTAwAAwJM54QYAAIAAwQ0AAAABghsAAAACBDcAAAAECG4AAAAIENwAAAAQILgBAAAgYP104Ov1Svz9adValxGMso4ZffW/oPdMzK+UEhn7S2ttuftn/CfPBHfjPTHWHvNd++0o+/OMPBPQ/0w44QYAAIAAwQ0AAAABghsAAAACBDcAAAAECG4AAAAIENwAAAAQILgBAAAgQHADAABAgOAGAACAAMENAAAAAa/3+/3+aODrtcym1to1ft/35c6O44hduxl9eOvf6pl4glLKx2PP81zurrUWuXb/j2eCu7nLe6Ln2e7dB766b4y+L6bGzuouzwT85DPhhBsAAAACBDcAAAAECG4AAAAIENwAAAAQILgBAAAgQHADAABAgOAGAACAAMENAAAAAYIbAAAAAgQ3AAAABAhuAAAACFiXybzf76uXMK1932Nz11pjc0OPUsrVSwAYynmeVy9h2vdEz9jWWtc6tm3rGg/MyQk3AAAABAhuAAAACBDcAAAAECC4AQAAIEBwAwAAQIDgBgAAgADBDQAAAAGCGwAAAAIENwAAAAQIbgAAAAh4vd/v90cDX68lpdb68dh932PrmNFxHENcu+T9kfLhrX+rz9yrlLLM5jzPq5cwlG3bfuzaPeGZ4Fnu8p7oebZn3PdH0VqLje+dO+UuzwT85DPhhBsAAAACBDcAAAAECG4AAAAIENwAAAAQILgBAAAgQHADAABAgOAGAACAAMENAAAAAYIbAAAAAgQ3AAAABKyJSYGfU0qJjP2ltRab++p5AWZmb/wZPe/BnrHAczjhBgAAgADBDQAAAAGCGwAAAAIENwAAAAQIbgAAAAgQ3AAAABAguAEAACBAcAMAAECA4AYAAIAAwQ0AAAABghsAAAAC1sSk/Jx935cR1FojY/lerbXo+ITeNZRSlhGk1jHCdwLMZZR9cRSpfbT3OtvPGcUoe0S76TPhhBsAAAACBDcAAAAECG4AAAAIENwAAAAQILgBAAAgQHADAABAgOAGAACAAMENAAAAAYIbAAAAAgQ3AAAABKzLjR3H0TV+3/fYWni2UsoygtbaEHMnr0fyM/bYtu3qJRC4F1P3rvsFnvdOnvHd9pPO81xm1PNd3f2304zPWoITbgAAAAgQ3AAAABAguAEAACBAcAMAAECA4AYAAIAAwQ0AAAABghsAAAACBDcAAAAECG4AAAAIENwAAAAQsCYmZUzHcXw8dt/36FqeprV29RL4YaWUq5dA4Nns+V57xp7n+fHYbds+HgtPeDbtt/NI7aEjSa2755nofU+M8r2Ujrlnehc64QYAAIAAwQ0AAAABghsAAAACBDcAAAAECG4AAAAIENwAAAAQILgBAAAgQHADAABAgOAGAACAAMENAAAAAWtiUoA/aa1dvQQYSiklMvYXz9s99d4Hsxnl8/U8P6OsmXsaZS8f5ZkoHXNffe2ccAMAAECA4AYAAIAAwQ0AAAABghsAAAACBDcAAAAECG4AAAAIENwAAAAQILgBAAAgQHADAABAgOAGAACAAMENAAAAAWtiUgAg4zzPrvGttdhaZuR6XHc9SynRtcCdn5/k3jXKvlg69oiesVd/PifcAAAAECC4AQAAIEBwAwAAQIDgBgAAgADBDQAAAAGCGwAAAAIENwAAAAQIbgAAAAgQ3AAAABAguAEAACBgXQZQa716CXyR7xBgTKWUq5dAQGstcg/0zPvLtm1LwnmesXu85zP2Xg9wf/2P989/OeEGAACAAMENAAAAAYIbAAAAAgQ3AAAABAhuAAAACBDcAAAAECC4AQAAIEBwAwAAQIDgBgAAgADBDQAAAAHrMplaa2zufd+XO+u5dsnrDAB8TWttqnl7bdv28dhSSnQtkDLK80aWE24AAAAIENwAAAAQILgBAAAgQHADAABAgOAGAACAAMENAAAAAYIbAAAAAgQ3AAAABAhuAAAACBDcAAAAECC4AQAAIGBNTMrPOY7j6iUAwDRaax+PLaUsT/vMM0p+vrvcA/BkpfM5/u49xQk3AAAABAhuAAAACBDcAAAAECC4AQAAIEBwAwAAQIDgBgAAgADBDQAAAAGCGwAAAAIENwAAAAQIbgAAAAhYlxurtS53t+/7x2OfcD0A4E9aa8vTbNt29RIecb/03lullL9YEaO7+x6TvG9Lx9wzXWcn3AAAABAguAEAACBAcAMAAECA4AYAAIAAwQ0AAAABghsAAAACBDcAAAAECG4AAAAIENwAAAAQILgBAAAgYE1MCgB3VEr5eGxrLTIvf3+dYSRPvHefsNel3hNP+A5bx/WY6do54QYAAIAAwQ0AAAABghsAAAACBDcAAAAECG4AAAAIENwAAAAQILgBAAAgQHADAABAgOAGAACAAMENAAAAAYIbAAAAAtbEpIyp1hoZC/AUrbXIWP6e6wyMxJ7EPznhBgAAgADBDQAAAAGCGwAAAAIENwAAAAQIbgAAAAgQ3AAAABAguAEAACBAcAMAAECA4AYAAIAAwQ0AAAABr/f7/U5MDAAAAE/mhBsAAAACBDcAAAAECG4AAAAIENwAAAAQILgBAAAgQHADAABAgOAGAACAAMENAAAAAYIbAAAAlu/3b1BHmTxdb6yJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_generated_images(images, n_cols=5):\n",
    "    n_rows = (len(images) + n_cols - 1) // n_cols\n",
    "    plt.figure(figsize=(2 * n_cols, 2 * n_rows))\n",
    "\n",
    "    for idx, img in enumerate(images):\n",
    "        plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        plt.imshow(img.squeeze(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate sample images (assuming you've done this step already)\n",
    "sample_images = generate_images(model, num_images=10, temperature=1.0)\n",
    "# Plot generated images\n",
    "plot_generated_images(sample_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
